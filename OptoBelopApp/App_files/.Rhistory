sum(is.na(OptoDF))
646*63
OptoDF$ID[is.na(OptoDF$TotalDoubleSupport..desviacion)]
OptoDF$Patology[is.na(OptoDF$TotalDoubleSupport..desviacion)]
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(MASS)
library(dplyr)
library(tidyr)
library(stringr)
library(rjson)
data.json <- fromJSON(file="datoshistorias.JSON")
pacientes.json <- data.json[["pacientes"]]
#Procesando todas las variables que nos interesan convirtiendolas en vectores para más tarde hacer un dataframe.
nombres <- lapply(pacientes.json, function(x) x["nombre"])
nombres <- lapply(nombres, function(x) x$nombre)
nombres[sapply(nombres, is.null)] <- NA #cambiando NULL por NA para evitar que las funciones hagan drop de esos valores.
nombres <- unlist(nombres, use.names = FALSE)
apellidos <- lapply(pacientes.json, function(x) x["apellidos"])
apellidos <- lapply(apellidos, function(x) x$apellidos)
apellidos[sapply(apellidos, is.null)] <- NA
apellidos <- unlist(apellidos, use.names = FALSE)
dni <- lapply(pacientes.json, function(x) x["dni"])
dni <- lapply(dni, function(x) x$dni)
dni[sapply(dni, is.null)] <- NA
dni <- unlist(dni, use.names = FALSE)
fechaNacimiento <- lapply(pacientes.json, function(x) x["fechaNacimiento"])
fechaNacimiento <- lapply(fechaNacimiento, function(x) x$fechaNacimiento)
fechaNacimiento[sapply(fechaNacimiento, is.null)] <- NA
fechaNacimiento <- unlist(fechaNacimiento, use.names = FALSE)
#Creo un vector con las historias de los pacientes.
historia <- lapply(pacientes.json, function(x) x[["camposPersonalizados"]])
historia <- lapply(historia, function(x)
if (length(x)>0){
j=0
for (i in 1:length(x)){ #Accedo a cada paciente y busco el campo personalizado "Historia".
if (x[[i]][["campo"]]=="Historia"){
j=i
}
}
if (j==0){NULL} #Si no hay Historia escribo un NULL
else{
gsub("<.*?>", "", x[[j]][["valor"]])} #uso gsub para eliminar los tags de html que hay en las historias
}
else{
NULL
}
)
historia[sapply(historia, is.null)] <- NA #cambio NULLs por NA
histdf <- data.frame("Historial"=matrix(historia)) #lo hago primero matriz para que no se hagan miles de variables, si no miles de muestras con una variable.
pacientesHistoria <- data.frame("Nombre"=nombres, "Apellidos"=apellidos, "FechaNac"=fechaNacimiento,"DNI"=dni, "Historial"=histdf)
pacientesHistoria <- data.frame(apply(pacientesHistoria,2,as.character)) #Historial me daba problemas por el formato lista, todo proviene del tipo LargeList, con esto lo soluciono.
#Quito las tildes
pacientesHistoria$Nombre <- iconv(pacientesHistoria$Nombre, from="UTF-8", to = "ASCII//TRANSLIT")
pacientesHistoria$Apellidos <- iconv(pacientesHistoria$Apellidos, from="UTF-8", to = "ASCII//TRANSLIT")
pacientesHistoria$Historial <- iconv(pacientesHistoria$Historial, from="UTF-8", to = "ASCII//TRANSLIT")
#Elimino los pacientes que no tengan historial grabado
pacientesHistoria <- subset(pacientesHistoria,!(is.na(pacientesHistoria$Historial) | pacientesHistoria$Historial == 'NA') )
write.csv2(pacientesHistoria,"pacientesHistoria.csv")
historias <- read.table("pacientesHistoria.csv",sep=";",dec=",",header = T, stringsAsFactors = T, na.strings = "NA")
historias <- subset(historias, select = -1)
pesos <- read.table("Pesos.csv",sep=";", header=T,stringsAsFactors = T)
pesos <- pesos[1:5]
colnames(pesos) <- c("Nombre", "Apellidos", "Peso(Kg)", "Altura(cm)", "N.Pie")
pesos$`Peso(Kg)`<- gsub(" Kg","",pesos$`Peso(Kg)`)
pesos$`Peso(Kg)`<- gsub(",",".",pesos$`Peso(Kg)`)
pesos$`Altura(cm)`<- gsub(" cm","",pesos$`Altura(cm)`)
pesos$`N.Pie`<- gsub(",",".",pesos$`N.Pie`)
pesos$`Peso(Kg)` <- as.numeric(pesos$`Peso(Kg)`)
pesos$`Altura(cm)` <- as.numeric(pesos$`Altura(cm)`)
pesos$`N.Pie` <- as.numeric(pesos$`N.Pie`)
#Borro las entradas que tenga el nombre nulo o los tres datos a la vez nulos.
pesos <- subset(pesos,!(is.na(pesos$Nombre) | (is.na(pesos$`Peso(Kg)`) & is.na(pesos$`Altura(cm)`) & is.na(pesos$`N.Pie`) ) ) )
#Quito las tildes
pesos$Nombre <- iconv(pesos$Nombre, from="UTF-8", to = "ASCII//TRANSLIT")
pesos$Apellidos <- iconv(pesos$Apellidos, from="UTF-8", to = "ASCII//TRANSLIT")
#Borro duplicados
pesos <- subset(pesos, !duplicated(paste(pesos$Nombre,pesos$Apellidos)))
write.csv2(pesos, "PesoAlturaPie.csv")
pesos <- read.table("PesoAlturaPie.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
#unión de solo los datos que coinciden de los dataframe, descartando los que no:
pacientesInnerJoin <- merge(historias, pesos, by = c('Nombre','Apellidos'))
#unión de ambos dataframes si coinciden; si no coinciden, solo se añaden aquellos provenientes del dataframe historial (puesto que tener historial es obligatorio para el entrenamiento del programa por lo que no nos sirven aquellos pacientes sin historial), es decir, guardamos los pacientes que no tenemos su peso, altura y nº de pie para poder trabajar con sus datos de opto
pacientesLeftJoin <-merge(historias, pesos, by = c('Nombre','Apellidos'), all.x=TRUE)
opto <- read.table("Optogait.csv", header=T, sep = ";",dec=",", fileEncoding= "latin1")
#crea un df con todos los registros de cada paciente unificados, sacando la media y la desviación típica
#Hago el dataframe que voy a unir al anterior para tener ya el dataset final.
opto <- mutate(opto,ApellidosyNombre = paste(opto$Apellidos, opto$Nombre))
# Calcular la media de todas las columnas para cada ApellidosyNombre
media_datos <- aggregate(. ~ ApellidosyNombre, data = opto[9:ncol(opto)], FUN = mean)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[9:ncol(opto)], FUN = sd)
# Unir los resultados
nuevo_datos <- merge(media_datos, desv_datos, by = "ApellidosyNombre", suffixes = c(".media", ".desviacion"))
# Borro las tildes de los nombres (para evitar futuras pérdidas de pacientes al fusionar los dataframes)
nuevo_datos$ApellidosyNombre <- iconv(nuevo_datos$ApellidosyNombre, from="UTF-8", to = "ASCII//TRANSLIT")
# Quito los espacios de los nombres y me quedo sólo con las tres primeras letras de cada nombre y apellidos para minimizar la cantidad de pacientes perdidos
# por ejemplo: Marta García Fernández -> margarfer
nuevo_datos$ApellidosyNombre <- lapply(nuevo_datos$ApellidosyNombre, function(x){
name <- ""
for (word in strsplit(x,split = " ")[[1]]){
name <- paste0(name,substr(word,start=1,stop=3))
}
x <- name}
)
nuevo_datos$ApellidosyNombre <- gsub(" ","",nuevo_datos$ApellidosyNombre)
#Tenemos un DF con la media y la desviación típica de cada variable.
#Junto las variables Nombre y Apellidos del anterior dataframe para poder unificarlo con el df de opto, le quito los espacios y me quedo con las tres primeras letras
pacientesInnerJoin <- mutate(pacientesInnerJoin,ApellidosyNombre = paste(pacientesInnerJoin$Apellidos, pacientesInnerJoin$Nombre))[-c(1,2,3,4)]
pacientesInnerJoin$ApellidosyNombre <- lapply(pacientesInnerJoin$ApellidosyNombre, function(x){
name <- ""
for (word in strsplit(x,split = " ")[[1]]){
name <- paste0(name,substr(word,start=1,stop=3))
}
x <- name}
)
pacientesInnerJoin$ApellidosyNombre <- gsub(" ","",pacientesInnerJoin$ApellidosyNombre)
#Merge de los datos de optogait con los otros dos
OptoDF <- merge(pacientesInnerJoin, nuevo_datos, by = c('ApellidosyNombre'))
#Datos que no coinciden para estudiar las pérdidas de pacientes y buscar una solución si fuera posible
datos_queNo_Coinciden <- merge(pacientesInnerJoin, nuevo_datos, by = c('ApellidosyNombre'), all = T)
datos_queNo_Coinciden <- datos_queNo_Coinciden[is.na(datos_queNo_Coinciden$Historial) | is.na(datos_queNo_Coinciden$`Paso.media`), ]
##Hay fallos de ortografía que me hacen eliminar varios pacientes ( que no son ni tildes ni mayúsculas, por ejemplo, una letra omitida)
ID <- 1:nrow(OptoDF)
OptoDF <- cbind(ID,OptoDF[2:ncol(OptoDF)])
OptoDF <- OptoDF %>%
mutate(Patology = case_when(
grepl("fasc",OptoDF$Historial,ignore.case=TRUE) ~ "Fascitis",
grepl( "esguince",OptoDF$Historial,ignore.case=TRUE) ~ "Esguince",
grepl( "pie plano|plano|PP",OptoDF$Historial,ignore.case=TRUE) ~ "Pie Plano",
grepl( "pie cavo|cavo|PC",OptoDF$Historial,ignore.case=TRUE) ~ "Pie Cavo",
grepl( "aquiles|aquilea",OptoDF$Historial,ignore.case=TRUE) ~ "Aquiles",
grepl( "predislocacion",OptoDF$Historial,ignore.case=TRUE) ~ "Sindrome de Predislocacion",
grepl( "varo",OptoDF$Historial,ignore.case=TRUE) ~ "Retropie Varo",
grepl( "valgo",OptoDF$Historial,ignore.case=TRUE) ~ "Retropie Valgo",
TRUE ~ "Otro"
), .before = 1)
OptoDF <- OptoDF[, -which(colnames(OptoDF)=='Historial')]
write.csv2(OptoDF, "OptoDF.csv")
OptoDF <- read.csv2("OptoDF.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
View(opto)
View(media_datos)
summary(media_datos)
summary(desv_datos)
View(desv_datos)
sd(1:2)^2
sd(1:2)
opto <- read.table("Optogait.csv", header=T, sep = ";",dec=",", fileEncoding= "latin1")
#crea un df con todos los registros de cada paciente unificados, sacando la media y la desviación típica
#Hago el dataframe que voy a unir al anterior para tener ya el dataset final.
opto <- mutate(opto,ApellidosyNombre = paste(opto$Apellidos, opto$Nombre))
# Calcular la media de todas las columnas para cada ApellidosyNombre
media_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = mean)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd(na.rm=TRUE))
opto <- read.table("Optogait.csv", header=T, sep = ";",dec=",", fileEncoding= "latin1")
#crea un df con todos los registros de cada paciente unificados, sacando la media y la desviación típica
#Hago el dataframe que voy a unir al anterior para tener ya el dataset final.
opto <- mutate(opto,ApellidosyNombre = paste(opto$Apellidos, opto$Nombre))
# Calcular la media de todas las columnas para cada ApellidosyNombre
media_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = mean)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd(opto[8:ncol(opto)],na.rm=TRUE))
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd(data,na.rm=TRUE))
mean(c(2,4,5,3,NA))
mean(c(2,4,5,3))
mean(c(2,4,5,3),na.rm = TRUE)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd, na.action = na.omit)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd, na.action = na.exclude)
# Calcular la media de todas las columnas para cada ApellidosyNombre
media_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = mean)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd, na.action = na.fail)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd, na.action = na.pass)
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = opto[8:ncol(opto)], FUN = sd, na.action = na.omit)
na.omit(c(3,4,5,NA,4))
na.omit(c(3,4,5,NA,4))[1]
na.omit(c(3,4,5,NA,4))[[1]]
a <- na.omit(c(3,4,2,NA))
a
a[1]
a[2]
a[3]
a
na.rm(c(3,4,5,NA,4))
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- aggregate(. ~ ApellidosyNombre, data = na.omit(opto[8:ncol(opto)]), FUN = sd)
summary(OptoDF)
OptoDF <- na.omit(OptoDF)
summary(OptoDF)
typeof(OptoDF$Patology)
OptoDF$Patology <- as.factor(OptoDF$Patology)
typeof(OptoDF$Patology)
summary(OptoDF)
class(OptoDF$Patology)
sum(is.na(OptoDF$Patology))
summary(OptoDF$Patology)
install.packages("tidyverse")
install.packages("palmerpenguins")
View(OptoDF)
setwd("C:/Users/Carlos/Desktop/Universidad/CUARTO/TFG/OptoBelopApp/App_files")
setwd("C:/Users/Carlos/Desktop/Universidad/CUARTO/TFG/OptoBelopApp/App_files")
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(MASS)
library(dplyr)
library(tidyr)
library(stringr)
library(rjson)
library(nnet)
library(graphics)
library(normalr)
data.json <- fromJSON(file="datoshistorias.JSON")
pacientes.json <- data.json[["pacientes"]]
#Procesando todas las variables que nos interesan convirtiendolas en vectores para más tarde hacer un dataframe.
nombres <- lapply(pacientes.json, function(x) x["nombre"])
nombres <- lapply(nombres, function(x) x$nombre)
nombres[sapply(nombres, is.null)] <- NA #cambiando NULL por NA para evitar que las funciones hagan drop de esos valores.
nombres <- unlist(nombres, use.names = FALSE)
apellidos <- lapply(pacientes.json, function(x) x["apellidos"])
apellidos <- lapply(apellidos, function(x) x$apellidos)
apellidos[sapply(apellidos, is.null)] <- NA
apellidos <- unlist(apellidos, use.names = FALSE)
dni <- lapply(pacientes.json, function(x) x["dni"])
dni <- lapply(dni, function(x) x$dni)
dni[sapply(dni, is.null)] <- NA
dni <- unlist(dni, use.names = FALSE)
fechaNacimiento <- lapply(pacientes.json, function(x) x["fechaNacimiento"])
fechaNacimiento <- lapply(fechaNacimiento, function(x) x$fechaNacimiento)
fechaNacimiento[sapply(fechaNacimiento, is.null)] <- NA
fechaNacimiento <- unlist(fechaNacimiento, use.names = FALSE)
#Creo un vector con las historias de los pacientes.
historia <- lapply(pacientes.json, function(x) x[["camposPersonalizados"]])
historia <- lapply(historia, function(x)
if (length(x)>0){
j=0
for (i in 1:length(x)){ #Accedo a cada paciente y busco el campo personalizado "Historia".
if (x[[i]][["campo"]]=="Historia"){
j=i
}
}
if (j==0){NULL} #Si no hay Historia escribo un NULL
else{
gsub("<.*?>", "", x[[j]][["valor"]])} #uso gsub para eliminar los tags de html que hay en las historias
}
else{
NULL
}
)
historia[sapply(historia, is.null)] <- NA #cambio NULLs por NA
histdf <- data.frame("Historial"=matrix(historia)) #lo hago primero matriz para que no se hagan miles de variables, si no miles de muestras con una variable.
historias <- read.table("pacientesHistoria.csv",sep=";",dec=",",header = T, stringsAsFactors = T, na.strings = "NA")
historias <- subset(historias, select = -1)
pesos <- read.table("PesoAlturaPie.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
#unión de solo los datos que coinciden de los dataframe, descartando los que no:
pacientesInnerJoin <- merge(historias, pesos, by = c('Nombre','Apellidos'))
#unión de ambos dataframes si coinciden; si no coinciden, solo se añaden aquellos provenientes del dataframe historial (puesto que tener historial es obligatorio para el entrenamiento del programa por lo que no nos sirven aquellos pacientes sin historial), es decir, guardamos los pacientes que no tenemos su peso, altura y nº de pie para poder trabajar con sus datos de opto
pacientesLeftJoin <-merge(historias, pesos, by = c('Nombre','Apellidos'), all.x=TRUE)
opto <- read.table("Optogait.csv", header=T, sep = ";",dec=",", fileEncoding= "latin1")
#crea un df con todos los registros de cada paciente unificados, sacando la media y la desviación típica
#Hago el dataframe que voy a unir al anterior para tener ya el dataset final.
opto <- mutate(opto,ApellidosyNombre = paste(opto$Apellidos, opto$Nombre))
# Calcular la media de todas las columnas para cada ApellidosyNombre
#Si uso data.table en vez de dataframe es más sencillo, simplente haciendo esto: opto[, sd(TContacto,na.rm=T), by="ApellidosyNombre"]
media_datos <- opto %>%
group_by(ApellidosyNombre) %>%
summarise(TContacto.media=mean(TContacto,na.rm=T),
TPaso.media=mean(TPaso,na.rm=T),
Ritmo.paso.media=mean(`Ritmo..paso.m.`,na.rm=T),
Paso.media=mean(Paso,na.rm=T),
Velocidad.media=mean(Velocidad,na.rm=T),
Aceleracion.media=mean(Aceleracion,na.rm=T),
Desequilibrio.media=mean(Desequilibrio,na.rm=T),
Distancia.media=mean(Distancia,na.rm=T),
Zancada.media=mean(Zancada,na.rm=T),
StrideTime.Cycle.media=mean(`StrideTime.Cycle`,na.rm=T),
SingleSupport.media=mean(SingleSupport,na.rm=T),
SingleSupport..media=mean(`SingleSupport.`,na.rm=T),
TotalDoubleSupport.media=mean(TotalDoubleSupport,na.rm=T),
TotalDoubleSupport..media=mean(`TotalDoubleSupport.`,na.rm=T),
TStance.media=mean(TStance,na.rm=T),
TStance..media=mean(`TStance.`,na.rm=T),
TSwing.media=mean(TSwing,na.rm=T),
TSwing..media=mean(`TSwing.`,na.rm=T),
ContactPhase.media=mean(ContactPhase,na.rm=T),
ContactPhase..media=mean(`ContactPhase.`,na.rm=T),
FootFlat.media=mean(FootFlat,na.rm=T),
FootFlat..media=mean(`FootFlat.`,na.rm=T),
PropulsivePhase.media=mean(`PropulsivePhase`,na.rm=T),
PropulsivePhase..media=mean(`PropulsivePhase.`,na.rm=T),
LoadResponse.media=mean(LoadResponse,na.rm=T),
LoadResponse..media=mean(`LoadResponse.`,na.rm=T),
PreSwing.media=mean(`PreSwing`,na.rm=T),
PreSwing..media=mean(`PreSwing.`,na.rm=T),
TContactoPerc.media=mean(TContactoPerc,na.rm=T))
# Calcular la desviación estándar de todas las columnas para cada ApellidosyNombre
desv_datos <- opto %>%
group_by(ApellidosyNombre) %>%
summarise(TContacto.desviacion=sd(TContacto,na.rm=T),
TPaso.desviacion=sd(TPaso,na.rm=T),
Ritmo.paso.desviacion=sd(`Ritmo..paso.m.`,na.rm=T),
Paso.desviacion=sd(Paso,na.rm=T),
Velocidad.desviacion=sd(Velocidad,na.rm=T),
Aceleracion.desviacion=sd(Aceleracion,na.rm=T),
Desequilibrio.desviacion=sd(Desequilibrio,na.rm=T),
Distancia.desviacion=sd(Distancia,na.rm=T),
Zancada.desviacion=sd(Zancada,na.rm=T),
StrideTime.Cycle.desviacion=sd(`StrideTime.Cycle`,na.rm=T),
SingleSupport.desviacion=sd(SingleSupport,na.rm=T),
SingleSupport..desviacion=sd(`SingleSupport.`,na.rm=T),
TotalDoubleSupport.desviacion=sd(TotalDoubleSupport,na.rm=T),
TotalDoubleSupport..desviacion=sd(`TotalDoubleSupport.`,na.rm=T),
TStance.desviacion=sd(TStance,na.rm=T),
TStance..desviacion=sd(`TStance.`,na.rm=T),
TSwing.desviacion=sd(TSwing,na.rm=T),
TSwing..desviacion=sd(`TSwing.`,na.rm=T),
ContactPhase.desviacion=sd(ContactPhase,na.rm=T),
ContactPhase..desviacion=sd(`ContactPhase.`,na.rm=T),
FootFlat.desviacion=sd(FootFlat,na.rm=T),
FootFlat..desviacion=sd(`FootFlat.`,na.rm=T),
PropulsivePhase.desviacion=sd(`PropulsivePhase`,na.rm=T),
PropulsivePhase..desviacion=sd(`PropulsivePhase.`,na.rm=T),
LoadResponse.desviacion=sd(LoadResponse,na.rm=T),
LoadResponse..desviacion=sd(`LoadResponse.`,na.rm=T),
PreSwing.desviacion=sd(`PreSwing`,na.rm=T),
PreSwing..desviacion=sd(`PreSwing.`,na.rm=T),
TContactoPerc.desviacion=sd(TContactoPerc,na.rm=T))
# Unir los resultados
nuevo_datos <- merge(media_datos, desv_datos, by = "ApellidosyNombre", suffixes = c(".media", ".desviacion"))
#De todos los datos nulos que hay, 55 son porque no se recogieron datos en esa prueba de optogait, es decir, una prueba vacía. Las eliminamos:
nuevo_datos <- subset(nuevo_datos,!is.na(nuevo_datos$TContacto.media))
# Borro las tildes de los nombres (para evitar futuras pérdidas de pacientes al fusionar los dataframes)
nuevo_datos$ApellidosyNombre <- iconv(nuevo_datos$ApellidosyNombre, from="UTF-8", to = "ASCII//TRANSLIT")
# Quito los espacios de los nombres y me quedo sólo con las tres primeras letras de cada nombre y apellidos para minimizar la cantidad de pacientes perdidos
# por ejemplo: Marta García Fernández -> margarfer
nuevo_datos$ApellidosyNombre <- lapply(nuevo_datos$ApellidosyNombre, function(x){
name <- ""
for (word in strsplit(x,split = " ")[[1]]){
name <- paste0(name,substr(word,start=1,stop=3))
}
x <- name}
)
nuevo_datos$ApellidosyNombre <- gsub(" ","",nuevo_datos$ApellidosyNombre)
#Tenemos un DF con la media y la desviación típica de cada variable.
#Junto las variables Nombre y Apellidos del anterior dataframe para poder unificarlo con el df de opto, le quito los espacios y me quedo con las tres primeras letras
pacientesInnerJoin <- mutate(pacientesInnerJoin,ApellidosyNombre = paste(pacientesInnerJoin$Apellidos, pacientesInnerJoin$Nombre))[-c(1,2,3,4)]
pacientesInnerJoin$ApellidosyNombre <- lapply(pacientesInnerJoin$ApellidosyNombre, function(x){
name <- ""
for (word in strsplit(x,split = " ")[[1]]){
name <- paste0(name,substr(word,start=1,stop=3))
}
x <- name}
)
pacientesInnerJoin$ApellidosyNombre <- gsub(" ","",pacientesInnerJoin$ApellidosyNombre)
#Merge de los datos de optogait con los otros dos
OptoDF <- merge(pacientesInnerJoin, nuevo_datos, by = c('ApellidosyNombre'))
#Datos que no coinciden para estudiar las pérdidas de pacientes y buscar una solución si fuera posible
datos_queNo_Coinciden <- merge(pacientesInnerJoin, nuevo_datos, by = c('ApellidosyNombre'), all = T)
datos_queNo_Coinciden <- datos_queNo_Coinciden[is.na(datos_queNo_Coinciden$Historial) | is.na(datos_queNo_Coinciden$`Paso.media`), ]
##Hay fallos de ortografía que me hacen eliminar varios pacientes ( que no son ni tildes ni mayúsculas, por ejemplo, una letra omitida)
View(OptoDF)
#Numerizo la salida:
#Fascitis: 1
#Esguince: 2
#Pie Plano: 3
#Pie Cavo: 4
#Aquiles: 5
#Síndrome de Predislocacion: 6
#Retropie Varo: 7
#Retropie Valgo: 8
#Otro: 9
OptoDF <- OptoDF %>%
mutate(Patology = case_when(
grepl("fasc",OptoDF$Historial,ignore.case=TRUE) ~ 1,
grepl( "esguince",OptoDF$Historial,ignore.case=TRUE) ~ 2,
grepl( "pie plano|plano|PP",OptoDF$Historial,ignore.case=TRUE) ~ 3,
grepl( "pie cavo|cavo|PC|vavo",OptoDF$Historial,ignore.case=TRUE) ~ 4,
grepl( "aquiles|aquilea",OptoDF$Historial,ignore.case=TRUE) ~ 5,
grepl( "predislocacion|pred",OptoDF$Historial,ignore.case=TRUE) ~ 6,
grepl( "varo",OptoDF$Historial,ignore.case=TRUE) ~ 7,
grepl( "valgo",OptoDF$Historial,ignore.case=TRUE) ~ 8,
TRUE ~ 9
), .before = 1)
#Anonimizo los datos
ID <- 1:nrow(OptoDF)
OptoDF <- cbind(ID,OptoDF)
PatologiaHistoria <- data.frame(Patology=OptoDF$Patology, Historia=OptoDF$Historial) #descomentar cuando se quiera hacer una comparación de las historias y la etiqueta definida
OptoDF <- select(OptoDF, -c(ApellidosyNombre,Historial))
OptoDF <- na.omit(OptoDF)
OptoDF$Peso.Kg.[which(OptoDF$Peso.Kg.==278)] <- 28 #Había una pesona de 278Kg que en realidad eran 28
write.csv2(OptoDF, "OptoDF.csv")
PatologiaHistoria[PatologiaHistoria$Patology==1,]$Historia
OptoDF <- read.csv2("OptoDF.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
View(OptoDF)
library(shiny); source('C:/Users/Carlos/Desktop/Universidad/CUARTO/TFG/OptoBelopApp/Launcher_App.R')
source('Launcher_App.R')
source('Launcher_App.R')
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(MASS)
library(dplyr)
library(tidyr)
library(stringr)
library(rjson)
library(nnet)
library(graphics)
library(normalr)
OptoDF <- read.csv2("OptoDF.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
OptoDF$Altura.cm.[which(OptoDF$Altura.cm..==283)] <- 183 #Había una pesona de 283cm que en realidad eran 183
write.csv2(OptoDF, "OptoDF.csv")
View(OptoDF)
OptoDF$Altura.cm.[which(OptoDF$Altura.cm..==283)] <- 183 #Había una pesona de 283cm que en realidad eran 183
OptoDF$Altura.cm.[which(OptoDF$Altura.cm.==283)] <- 183 #Había una pesona de 283cm que en realidad eran 183
write.csv2(OptoDF, "OptoDF.csv")
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(MASS)
library(dplyr)
library(tidyr)
library(stringr)
library(rjson)
library(nnet)
library(graphics)
library(normalr)
OptoDF <- read.csv2("OptoDF.csv",sep=";",dec=",",header = T, stringsAsFactors = T)[-1]
measure$predict_type
library(mlr3learners)
library(mlr3viz)
library(mlr3tuning)
library(BBmisc)
data <- OptoDF[-1]
data$Patology <- as.factor(data$Patology)
normData <- normaliseData(data[-1],getLambda(data[-1], parallel = TRUE))
normData <- mutate(data[1],normData)
#creo un mlr3 task, que es el dataframe pero en un formato mlr3
task_Opto = as_task_classif(normData, target = "Patology", id="pieses")
#Hago las particiones
splits = partition(task_Opto)
#Elijo el algoritmo que voy a usar y como quiero la predicción si en probabilidades ("prob") o en clases ("response")
learner_rpart = lrn("classif.cv_glmnet", predict_type="response")
# param_set$set_values(<par1> = <value1>, ...)
learner_rpart$param_set
#creo el método de resample que voy a usar
resampling = rsmp("repeated_cv", folds=5, repeats=2)
#Lo aplico
rr = resample(task = task_Opto, learner = learner_rpart, resampling = resampling)
#Veo los resultados estadísticos
rr$aggregate(msr("classif.acc"))
#para ver los resultados en cada iteración
acc = rr$score(msr("classif.acc"))
acc[, .(iteration, classif.acc)]
autoplot(predictions)
as.data.table(mlr_measures)[c(3,10)]
measure <- msr("classif.acc")
predictions$score(measure)
measure$predict_type
acc[, .(iteration, classif.acc)]
learner_rpart$train(task_Opto,splits$train)
task_Opto = as_task_classif(normData, target = "Patology", id="pieses")
splits = partition(task_Opto)
learner_rpart = lrn("classif.cv_glmnet", predict_type="response")
learner_rpart$train(task_Opto,splits$train)
predictions = learner_rpart$predict(task_Opto, splits$test) # En mlr3 no se puede cambiar el parámetro s (es decir, el lambda que vamos a usar para las predicciones), supuestamente usa el óptimo por defecto, por eso deberíamos olvidarnos de tal cosa.
autoplot(predictions)
#hago un df quitando valores de la clase 9 ("otro")
dataotro <-  slice_sample(normData[which(normData$Patology==9),],n=150)
dataotro <- anti_join(normData,dataotro)
task_Opto = as_task_classif(normData, target = "Patology", id="pieses")
splits = partition(task_Opto)
learner_rpart = lrn("classif.cv_glmnet", predict_type="response")
learner_rpart$train(task_Opto,splits$train)
predictions = learner_rpart$predict(task_Opto, splits$test) # En mlr3 no se puede cambiar el parámetro s (es decir, el lambda que vamos a usar para las predicciones), supuestamente usa el óptimo por defecto, por eso deberíamos olvidarnos de tal cosa.
autoplot(predictions)
task_Opto = as_task_classif(dataotro, target = "Patology", id="pieses")
#hago un df quitando valores de la clase 9 ("otro")
dataotro <-  slice_sample(normData[which(normData$Patology==9),],n=150)
dataotro <- anti_join(normData,dataotro)
task_Opto = as_task_classif(dataotro, target = "Patology", id="pieses")
splits = partition(task_Opto)
learner_rpart = lrn("classif.cv_glmnet", predict_type="response")
learner_rpart$train(task_Opto,splits$train)
predictions = learner_rpart$predict(task_Opto, splits$test) # En mlr3 no se puede cambiar el parámetro s (es decir, el lambda que vamos a usar para las predicciones), supuestamente usa el óptimo por defecto, por eso deberíamos olvidarnos de tal cosa.
autoplot(predictions)
table(dataotro$Patology)
autoplot(predictions)
normData <- normaliseData(data[-1],getLambda(data, parallel = TRUE))
normData <- mutate(data[1],normData)
resampling = rsmp("cv", folds=5)
measure=msr("classif.mauc_aunp") #La medida por la cual se va a hiperparametrizar (decidir qué parámetros son los buenos cuando esta medida sea la mejor)
#creo un mlr3 task, que es el dataframe pero en un formato mlr3
task_Opto = as_task_classif(normData, target = "Patology", id="pieses")
learner = lrn("classif.svm",
cost  = to_tune(1e-1, 1e5),
gamma = to_tune(1e-1, 1),
predict_type  = "prob",
type="C-classification",
kernel = "radial"
)
instance = ti(
task=task_Opto,
learner=learner,
resampling=resampling,
measures=measure,
terminator = trm("none") #Uso trm none para hacer una búsqueda de hiperparámetros exhaustiva, que se para sola al ser un grid search.
)
# Ahora elegimos un método que hará la selección de hiperparámetros, (Random Search y Grid Search
# buscan casi todas las posibilidades (Grid search busca todas) y después hay algoritmos más sofisticados que en función del anterior resultado,
# varían los hiperparámetros de una manera u otra como Iterative Racing,
# Covariance Matrix Adaptation... estos últimos son más sofisticados)
tuner = tnr("grid_search", resolution = 5, batch_size = 10) #Resolution es el nº de valores que se prueban por cada hiperparámetro y batch_size es cuántas configuraciones se evalúan al mismo tiempo
tuner$optimize(instance)
